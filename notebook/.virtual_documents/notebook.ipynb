import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay, classification_report, roc_curve

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
# from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline
from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder 


# !pip install xgboost
# !python.exe -m pip install --upgrade pip


df = pd.read_csv("../data_source/drug.csv")
df.head()


df.info()





df.isnull().sum()


df.Sex.value_counts()#unique()


df.describe()


df.Drug.value_counts()#unique()


sns.histplot(data=df, kde=True)


sns.countplot(data=df, x="BP", hue="Sex")


sns.countplot(data=df,x="Drug", hue="Sex")


num=[]
cat=[]
for i in df.columns:
    if df[i].dtype == 'object':
        cat.append(i)
    else:
        num.append(i)


# data split

# X = df.drop("Drug", axis=1).values
# y = df.Drug.values
X = df.drop(["Drug"], axis=1)
y = df["Drug"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)


x_train = pd.get_dummies(X_train)
x_test = pd.get_dummies(X_test)


x_train


clf = LogisticRegression()
clf.fit(x_train, y_train)

y_pred = clf.predict(x_test)

print('Model: ',clf)
print(f"Accuracy score: {accuracy_score(y_test, y_pred):.3f}")
print("\nClassification Report:", classification_report(y_test, y_pred))
cm=confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot=True, fmt='g')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()


# logistcreg with params
clfr = LogisticRegression(solver='liblinear', max_iter=5000)
clfr.fit(x_train, y_train)

y_pred = clfr.predict(x_test)

print('Model: ',clfr)
print(f"Accuracy score: {accuracy_score(y_test, y_pred):.3f}")
print("\nClassification Report:", classification_report(y_test, y_pred))
cm=confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot=True, fmt='g')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()



### SMOTE
from imblearn.over_sampling import SMOTE
X_train_sm, y_train_sm = SMOTE().fit_resample(x_train, y_train)
X_train_sm.shape,y_train_sm.shape


sns.countplot(y=y_train_sm, data=df)
plt.ylabel('Drug Type')
plt.xlabel('Total')
plt.show()


# X_train_sm, y_train_sm, x_test, y_test


#logisticcReg SMOTE
clfsm = LogisticRegression()
clfsm.fit(X_train_sm, y_train_sm)

y_pred = clfr.predict(x_test)

print('Model: ',clfr)
print(f"Accuracy score: {accuracy_score(y_test, y_pred):.3f}")
print("\nClassification Report:", classification_report(y_test, y_pred))
cm=confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot=True, fmt='g')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()


#logisticcReg SMOTE with params
clfsmpa = LogisticRegression(solver='liblinear', max_iter=5000)
clfsmpa.fit(X_train_sm, y_train_sm)

y_pred = clfsmpa.predict(x_test)

print('Model: ',clfsmpa)
print(f"Accuracy score: {accuracy_score(y_test, y_pred):.3f}")
print("\nClassification Report:", classification_report(y_test, y_pred))
cm=confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot=True, fmt='g')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()


# try pipeline
x=X.copy()
y=y.copy()

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)
x_train.shape, y_train.shape

cat_col = [1,2,3]
num_col = [0,4]

# Define the transformer
transform = ColumnTransformer(
    transformers=[
        ("encoder", Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing categorical values
            ('encoder', OrdinalEncoder()) 
        ]), cat_col),
        
        ("num_transform", Pipeline(steps=[
            ('imputer', SimpleImputer(strategy="median")),  # Impute missing numeric values
            ('scaler', StandardScaler())  
        ]), num_col),
    ],
    remainder='passthrough'  # Keep other columns as is
)

# Define the pipeline
pipeline = Pipeline(steps=[
    ('transform', transform),
    ('smote', SMOTE()),  # Apply SMOTE for oversampling
    ('classifier', LogisticRegression(solver='liblinear', max_iter=5000))  # Logistic regression
])


# Fit the pipeline
pipeline.fit(x_train, y_train)


y_pred = pipeline.predict(x_test)

# print('Model: ',pipeline)
print('Model')
print(f"Accuracy score: {accuracy_score(y_test, y_pred):.3f}")
print("\nClassification Report:", classification_report(y_test, y_pred))
cm=confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot=True, fmt='g')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()



import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.model_selection import train_test_split
from imblearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import CategoricalNB, GaussianNB
from sklearn.tree import DecisionTreeClassifier


# Models to evaluate
models = {
    "Random Forest": RandomForestClassifier(max_leaf_nodes=30),
    "KNeighborsClassifier": KNeighborsClassifier(n_neighbors=20),
    "SVC": SVC(kernel='linear', max_iter=251),
    "DecisionTreeClassifier": DecisionTreeClassifier(max_leaf_nodes=20),
}

for model_name, model in models.items():
    print(model_name, model)



# try pipeline
xp=X.copy()
yp=y.copy()

X_train, X_test, y_train, y_test = train_test_split(xp, yp, test_size = 0.3, random_state = 42)
X_train.shape, y_train.shape

cat_col = [1,2,3]
num_col = [0,4]

# Define the transformer
transform = ColumnTransformer(
    transformers=[
        ("encoder", Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing categorical values
            ('encoder', OrdinalEncoder()) 
        ]), cat_col),
        
        ("num_transform", Pipeline(steps=[
            ('imputer', SimpleImputer(strategy="median")),  # Impute missing numeric values
            ('scaler', StandardScaler())  
        ]), num_col),
    ],
    remainder='passthrough'  # Keep other columns as is
)


# Loop through each model and evaluate
for model_name, model in models.items():
    print(f"\nEvaluating {model_name}...")
        
    # Define the pipeline
    pipeline = Pipeline(steps=[
        ('transform', transform),
        ('smote', SMOTE()),  # Apply SMOTE for oversampling
        ('classifier', model)  # Use the current model
        ])

    # Fit the pipeline
    pipeline.fit(X_train, y_train)

    y_pred = pipeline.predict(X_test)

    # print('Model: ',pipeline)
    print('Model')
    print(f"Accuracy score: {accuracy_score(y_test, y_pred):.3f}")
    print("\nClassification Report:", classification_report(y_test, y_pred))
    cm=confusion_matrix(y_test,y_pred)
    sns.heatmap(cm, annot=True, fmt='g')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()



## GridSearch
from sklearn.model_selection import GridSearchCV
# try pipeline
xg=X.copy()
yg=y.copy()

X_train, X_test, y_train, y_test = train_test_split(xg, yg, test_size = 0.3, random_state = 42)
X_train.shape, y_train.shape

cat_col = [1,2,3]
num_col = [0,4]

results={}

# Define the transformer
transform = ColumnTransformer(
    transformers=[
        ("encoder", Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing categorical values
            ('encoder', OrdinalEncoder()) 
        ]), cat_col),
        
        ("num_transform", Pipeline(steps=[
            ('imputer', SimpleImputer(strategy="median")),  # Impute missing numeric values
            ('scaler', StandardScaler())  
        ]), num_col),
    ],
    remainder='passthrough'  # Keep other columns as is
)

models_grids = {
    "Random Forest": RandomForestClassifier(),
    "KNeighborsClassifier": KNeighborsClassifier(),
    "SVC": SVC(),
    "DecisionTreeClassifier": DecisionTreeClassifier(),
}

param_grids = {
    "Random Forest": {
        'classifier__n_estimators': [50, 100, 200],
        'classifier__max_depth': [10, 20, 30],
        'classifier__min_samples_split': [2, 5, 10]
    },
    "KNeighborsClassifier": {
        'classifier__n_neighbors': [3, 5, 7],
        'classifier__weights': ['uniform', 'distance'],
        'classifier__metric': ['euclidean', 'manhattan']
    },
    "SVC": {
        'classifier__C': [0.1, 1, 10],
        'classifier__kernel': ['linear', 'rbf'],
        'classifier__gamma': ['scale', 'auto']
    },
    "DecisionTreeClassifier": {
        'classifier__max_depth': [10, 20, 30],
        'classifier__min_samples_split': [2, 5, 10]
    }
}

# Loop through each model and evaluate
for model_name, model in models_grids.items():
    print(f"\nEvaluating {model_name} with GridSearchCV...")
        
    # Define the pipeline
    pipeline = Pipeline(steps=[
        ('transform', transform),
        ('smote', SMOTE()),  # Apply SMOTE for oversampling
        ('classifier', model)  # Use the current model
        ])

    # Apply GridSearchCV
    grid_search = GridSearchCV(pipeline, param_grid=param_grids[model_name], cv=3, n_jobs=-1, verbose=1)
        
    # Fit the model using GridSearchCV
    grid_search.fit(x_train, y_train)

    # Best hyperparameters found by GridSearchCV
    best_params = grid_search.best_params_
    print(f"Best parameters for {model_name}: {best_params}")

    # Get the best model from the grid search
    best_model = grid_search.best_estimator_

    # Predict and evaluate using the best model
    y_pred = best_model.predict(x_test)
        
    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average="macro")
    print("Accuracy: ", str(round(accuracy, 2) * 100) + "%", "F1: ", round(f1, 2))


    print('Model')
    print(f"Accuracy score: {accuracy_score(y_test, y_pred):.3f}")
    print("\nClassification Report:", classification_report(y_test, y_pred))
    cm=confusion_matrix(y_test,y_pred)
    sns.heatmap(cm, annot=True, fmt='g')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

    # Store results
    results[model_name] = {
        "accuracy": accuracy,
        "f1":f1,
        "best_params": best_params
        }



results


predictions = pipe.predict(X_test)
cm = confusion_matrix(y_test, predictions, labels=pipe.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipe.classes_)
disp.plot()
plt.savefig("Results/model_results.png", dpi=120)


## GridSearch
from sklearn.model_selection import GridSearchCV
# try pipeline
xr=X.copy()
yr=y.copy()

X_train, X_test, y_train, y_test = train_test_split(xr, yr, test_size = 0.3, random_state = 42)
X_train.shape, y_train.shape

cat_col = [1,2,3]
num_col = [0,4]

results={}

# Define the transformer
transform = ColumnTransformer(
    transformers=[
        ("encoder", Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing categorical values
            ('encoder', OrdinalEncoder()) 
        ]), cat_col),
        
        ("num_transform", Pipeline(steps=[
            ('imputer', SimpleImputer(strategy="median")),  # Impute missing numeric values
            ('scaler', StandardScaler())  
        ]), num_col),
    ],
    remainder='passthrough'  # Keep other columns as is
)

models_grids = {
    "Random Forest": RandomForestClassifier(),
}

param_grids = {
    "Random Forest": {
        'classifier__n_estimators': [50, 100, 200],
        'classifier__max_depth': [10, 20, 30],
        'classifier__min_samples_split': [2, 5, 10]
    },
}

# Loop through each model and evaluate
for model_name, model in models_grids.items():
    print(f"\nEvaluating {model_name} with GridSearchCV...")
        
    # Define the pipeline
    pipeline = Pipeline(steps=[
        ('transform', transform),
        ('smote', SMOTE()),  # Apply SMOTE for oversampling
        ('classifier', model)  # Use the current model
        ])

    # Apply GridSearchCV
    grid_search = GridSearchCV(pipeline, param_grid=param_grids[model_name], cv=3, n_jobs=-1, verbose=1)
        
    # Fit the model using GridSearchCV
    grid_search.fit(X_train, y_train)

    # Best hyperparameters found by GridSearchCV
    best_params = grid_search.best_params_
    print(f"Best parameters for {model_name}: {best_params}")

    # Get the best model from the grid search
    best_model = grid_search.best_estimator_

    # Predict and evaluate using the best model
    y_pred = best_model.predict(X_test)
        
    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average="macro")
    print("Accuracy: ", str(round(accuracy, 2) * 100) + "%", "F1: ", round(f1, 2))


    print('Model')
    print(f"Accuracy score: {accuracy_score(y_test, y_pred):.3f}")
    print("\nClassification Report:", classification_report(y_test, y_pred))
    cm=confusion_matrix(y_test,y_pred)
    sns.heatmap(cm, annot=True, fmt='g')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

    # Store results
    results[model_name] = {
        "accuracy": accuracy,
        "f1":f1,
        "best_params": best_params,
        "best_model":best_model,
        }






# Model prediction tester
x_input=X.copy()
y_input=y.copy()

test=x_input.loc[30]
print('test input\n', test)
print('true value: ',y_input.iloc[30])

testset=test.values
user_input=testset.reshape(1, -1)
# test1
pred=best_model.predict(user_input)[0]
print('prediction: ', pred)


feature_names = x_input.columns.tolist()
feature_names


# Function to input user data and predict

def predict_user_input(Age, Sex, BP, Cholesterol, Na_to_K):
    # Define the feature names (you should replace these with the actual feature names in your dataset)
    feature_names = x_input.columns.tolist()

    # Map user inputs to the correct feature order and types
    user_input = [Age, Sex, BP, Cholesterol, Na_to_K]

    # Convert user input into a DataFrame with the same structure as X
    user_input_df = pd.DataFrame([user_input], columns=feature_names)

    # Predict the drug class using the trained model
    prediction = best_model.predict(user_input_df)

    # Output the predicted class
    print(f"\nPredicted Drug Classification: {prediction[0]}")




# Example usage
predict_user_input(30, "M", "HIGH", "NORMAL", 15.4)



